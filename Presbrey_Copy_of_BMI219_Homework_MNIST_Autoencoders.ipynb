{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Presbrey Copy of BMI219_Homework_MNIST_Autoencoders.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/003084-K/cardsort/blob/master/Presbrey_Copy_of_BMI219_Homework_MNIST_Autoencoders.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAWVbhB16p-U",
        "colab_type": "text"
      },
      "source": [
        "# Homework 1: Autoencoding MNIST and Celebrity Faces\n",
        "\n",
        "**Due Date: May 1st, 2020 @ 10:00am (before class)**\n",
        "\n",
        "Please turn in this completed notebook and requested material to kangway@keiserlab.org.\n",
        "\n",
        "**Collaboration Policy and More** \n",
        "\n",
        "You're welcome (and highly encouraged) to work with and discuss this homework assignment with others in the class, and feel free to use any resources (textbooks, online notebooks, etc). The only requirement is that the final notebook that you turn in must be your own written work (no copy and pasting, please).\n",
        "\n",
        "**Overview**\n",
        "\n",
        "In class, we covered how Hinton and Salakhutdinov's 2006 Science Paper, [\"Reducing the Dimensionality of Data with Neural Networks\"](https://www.cs.toronto.edu/~hinton/science.pdf) was one of the first demonstrations of unsupervised pretraining for use in training deep neural networks. In this homework, we'll implement autoencoders in the context of MNIST. Additionally, as an optional assignment, a similar architecture can be used for a subset of CelebA on celebrity faces provided in the same directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzZTVHKCt6RH",
        "colab_type": "code",
        "outputId": "24d10018-b466-4ec9-9c54-f281af7c815e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "px8y6lmq6p-V",
        "colab_type": "text"
      },
      "source": [
        "## Before you get started\n",
        "\n",
        "**1) Background Reading**\n",
        "\n",
        "Please Read Hinton and Salakhutdinov's 2006 seminal work on deep autoencoders (https://www.cs.toronto.edu/~hinton/science.pdf), as this notebook aims to recreate this important work. A few questions to think about as you read that will help you in this assignment:\n",
        "  - What architecture do they use for their deep autoencoders?\n",
        "  - Why were deep neural networks so much harder to train in 2006?\n",
        "\n",
        "**2) How to run this notebook**\n",
        "\n",
        "This Jupyter Notebook can be used in two ways:\n",
        "* *Option 1: Download the notebook*\n",
        "\n",
        "  We've included all the imports necessary for this homework. Please make sure you're running Python 3 with PyTorch (and Torchvision) 1.0 installed and ready to go, along with NumPy and Matplotlib. Although you might find that these models train a bit faster on GPU, this homework assignment should be doable on most modern laptops. If you're having trouble please let us know ASAP.\n",
        "\n",
        "* *Option 2: Run it online on Google Colaboratory*\n",
        "\n",
        "  - Colab gives access to a GPU, so it could be useful in case you don't have CUDA installed on your computer (**Note: you can use this as an opportunity to get started on GPU training, but we recomend you develop your model and make sure everything works on CPU first**)\n",
        "  - Make a copy of this notebook in your Google Drive folder: \"File\" -> \"Save a copy in Drive...\"\n",
        "  - By default, Colab does not make GPUs available, but you can easily access them by selecting GPU in \"Runtime\" -> \"Change runtime type...\"\n",
        "  - Remember that Colab runs in a temporary virtual machine, so all the data created while running the notebook will be lost at the end of the session, or when the runtime disconnects due to inactivity. We provide functions to download and re-upload data.\n",
        "  - If you don't want to keep downloading/uploading files, you can very easily link Colab to your personal Google Drive by mounting it on your runtime, see instructions [here](https://colab.research.google.com/notebooks/io.ipynb#scrollTo=u22w3BFiOveA).\n",
        "\n",
        "**3) How to complete this assignment**\n",
        "\n",
        "  - Fill out the relevant code blocks as indicated\n",
        "  - Answer questions by writing them directly in the text block. Please keep your written answers concise, most are meant to be answered in a sentence or two.\n",
        "  - Provide your best trained model along with intermediate checkpoints, as explained below.\n",
        "\n",
        "**4) Optional exercise: CelebA Data** \n",
        "\n",
        "Whereas MNIST is a toy dataset built into PyTorch, we can also examine a more complex feature space using a subset of 90,000 celebrity portraits from CelebA (see [Liu et al. (2014), \"Deep Learning Face Attributes in the Wild\"](https://arxiv.org/abs/1411.7766)). This is an optional part of the homework, but is a nice way to see how autoencoders perform on other types of visual data. There will be a .zip file of the relevant celebrity faces dataset on the Google Classroom link.\n",
        "\n",
        "***Let's start!***\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykN_eZdJ6p-f",
        "colab_type": "text"
      },
      "source": [
        "## Train an autoencoder on MNIST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgkNXdEr6p-X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Import all the necessary libraries\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "from IPython.display import Image, display\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import save_image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "es5SJPnnVISe",
        "colab_type": "text"
      },
      "source": [
        "You shouldn't need CUDA for this assignment, but if you want a head start, or if you just want to see the difference between using a CPU versus a GPU, set `use_cuda = True` below. \n",
        "You can check if CUDA is available on your computer with: `torch.cuda.is_available()`\n",
        "\n",
        "If you are working on Colab, make sure to activate the GPU (\"Runtime\" -> \"Change runtime type...\")."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EKT2zNJR6Rwj",
        "colab": {}
      },
      "source": [
        "use_cuda = False\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "28wNsWmC6Q63",
        "colab": {}
      },
      "source": [
        "torch.manual_seed(7);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBrzn_tnFKsM",
        "colab_type": "text"
      },
      "source": [
        "> **Question 0.1) Why is it important to set the seed for the random number generator?**\n",
        "\n",
        "*Double-click to add your answer...*\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUNoP7qY6p-g",
        "colab_type": "text"
      },
      "source": [
        "### 1. MNIST Dataset\n",
        "\n",
        "As noted in class, MNIST has been widely used to benchmark new deep learning architectures and is already built into PyTorch. We provide this data as a starting point, again noting that the mean and std of the training set are calculated to be 0.1307 and 0.3081, respectively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8H20Wcmu6p-h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preprocessing = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.MNIST(\n",
        "    './bmi219_downloads', train=True, download=True,\n",
        "    transform=preprocessing)\n",
        "                               \n",
        "test_dataset = datasets.MNIST(\n",
        "    './bmi219_downloads',train=False, download=True, \n",
        "    transform=preprocessing)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnPPEsT96p-l",
        "colab_type": "text"
      },
      "source": [
        "> **Q1.1) How many examples do the training set and test set have?**\n",
        "\n",
        "...\n",
        "\n",
        "> **Q1.2) What's the format of each input example? Can we directly put these into a fully-connected layer?**\n",
        "\n",
        "...\n",
        "\n",
        "> **Q1.3) Why do we normalize the input data for neural networks?**\n",
        "\n",
        "...\n",
        "\n",
        "> **Q1.4) In this scenario, MNIST is already split into a training set and a test set. What is the purpose of dataset splitting (and specifically, the purpose of a test set)? For modern deep learning, a three-way split into training, validation, and test sets is usually preferred, why?**\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vr83wABJ6p-m",
        "colab_type": "text"
      },
      "source": [
        "### 2. Using DataLoaders for MNIST\n",
        "\n",
        "Set up the DataLoader objects below. Although the arguments are prepopulated, you may need to change the batch sizes or other arguments during training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Df46Ok2t6p-n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 1  ### Please change this as necessary\n",
        "NUM_WORKERS = 1  ### Use more workers for more CPU threads"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tx21NwCGqwE-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE, \n",
        "    shuffle=True,\n",
        "    num_workers=NUM_WORKERS)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=BATCH_SIZE, \n",
        "    shuffle=False,\n",
        "    num_workers=NUM_WORKERS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jL6Zs5z6p-r",
        "colab_type": "text"
      },
      "source": [
        "As you begin training your model, you may need to adjust the batch size or experiment with several of these different parameters.\n",
        "\n",
        "> **Q2.1) It's recommended to shuffle the training data over each epoch, but this isn't typically the case for the test set, why?**\n",
        "\n",
        "...\n",
        "\n",
        "> **Q2.2) What seems to be a good batch size for training? What happens if you train with a batch size of 1? What about a batch size equal to the total training set?**\n",
        "\n",
        "...\n",
        " \n",
        "> **Q2.3) The PyTorch DataLoader object is an iterator that generates batches as it's called. Try to pull a few images from the training set to see what these images look like. Does the DataLoader return only the images? What about the labels?**\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMlogSbL6p-s",
        "colab_type": "text"
      },
      "source": [
        "### 3. Define your neural network architecture\n",
        "\n",
        "With your data and dataloaders appropriately set, you're ready to define a network architecture. In this homework, we'll ask you to evaluate two different architectures.\n",
        "\n",
        "For the first (we'll call it HNet in this homework), please implement Hinton's 2006 architecture of 7-hidden layers:\n",
        "\n",
        "```[1000 x 500 x 250 x 2 x 250 x 500 x 1000]. ```\n",
        "\n",
        "For the second, implement your own autoencoder architecture, again using a bottleneck dimension of 2. As a note, the larger your model, the longer it will take to train. Can you achieve similar performance to the model above using a more condensed model?\n",
        "\n",
        "(As a reminder, what is the size of the inputs to these neural networks?)\n",
        "\n",
        "Try to vary the type of activation functions (tanh, sigmoid, relu, etc). You may also find building modular Sequential Layers helpful (nn.Sequential)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGK2j8UM6p-t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class HNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(HNet, self).__init__()\n",
        "        ### Implement a version of Hinton's 2006 Autoencoder\n",
        "        ### Using a bottleneck latent dimension of 2\n",
        "        pass\n",
        "\n",
        "    def forward(self, x):\n",
        "        ### *** Fill this in as necessary ***\n",
        "        pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_Grvigg6p-x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MyNet, self).__init__()\n",
        "        ### Fill in your network architecture here!\n",
        "        pass\n",
        "    \n",
        "    def forward(self, x):\n",
        "        ### *** Fill this in as necessary ***\n",
        "        pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqQ-CAnlYad2",
        "colab_type": "text"
      },
      "source": [
        "> **Q3.1) What activation functions did you use, and why?**\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2rGw2FD6p-2",
        "colab_type": "text"
      },
      "source": [
        "### 4. Write your own training function\n",
        "\n",
        "Write your own training function that takes your **model**, an **optimizer**, and a **training criterion**, and iterates over the **training set**. \n",
        "* *Hint*: Because an autoencoder is a form of unsupervised learning, we won't need to use the labels like in the MNIST classification example. Keep in mind the format of the images and whether they're compatible with feed-forward networks.\n",
        "* For each epoch, print and record (in an array or list) the training loss.\n",
        "* You may want to store the model and its weights on file at regular intervals (\"checkpoints\"). In order to visualize the autoencoder's learning process, we suggest to save at least three timepoints: early, intermediate, and final (for instance, if your model converges after 60 epochs, save your model at 5 epochs, 30 epochs, and 60 epochs). \n",
        "* **Save your best trained model and provide those checkpoints along with your final homework. There are more directions on uploading/download, and also saving directly to your Google Drive folder below.**\n",
        "\n",
        "As a side note, PyTorch offers tremendous flexibility in how to train your neural networks. Although we'll write our own training function here, there are many ways to do this: as a method written into your Net() class, as its own Class, etc.\n",
        "\n",
        "A few other useful tips: \n",
        "- Feel free to look at the MNIST notebooks on dense networks, as well as with CNNs as a guide.\n",
        "- Printing out the intermediate variables to understand whats happening in each step can be helpful. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2vn3O8L6p-2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(train_loader, model, optimizer, criterion, \n",
        "          n_epochs=10, **kwargs):\n",
        "    \"\"\"\n",
        "    Define your training loop in this function\n",
        "    \"\"\"\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsMeeItp6p-6",
        "colab_type": "text"
      },
      "source": [
        "### 5. Define your optimization and evaluation criterion\n",
        "\n",
        "Define an optimizer and criterion (loss function) for your neural network training. To setup your optimizer, you'll have to instantiate your models above, and choose a learning rate. Try a few different optimizers and learning rates to get a sense of what will train within a reasonable timeframe (if your deep network isn't too deep, reaching convergence shouldn't take more than 5-10 minutes with the right choice of learning rate and optimizer)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WQBWlyJcj7w",
        "colab_type": "text"
      },
      "source": [
        "> **Q5.1) What loss function is suited to this problem?**\n",
        "\n",
        "...\n",
        "\n",
        "> **Q5.2) Try a few optimizers, what seemed to work best?**\n",
        "\n",
        "...\n",
        "\n",
        "> **Q5.3) What's the effect of choosing different batch sizes?**\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftxeVJJP6p-7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Instantiate your model\n",
        "### Define your loss function (training criterion)\n",
        "### Choose your optimizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zudx4FKI6p_C",
        "colab_type": "text"
      },
      "source": [
        "### 6. Run your training loop and start training\n",
        "\n",
        "It's a great idea to monitor the early epochs of your training (\"babysit your training\") to keep an eye on learning. Does the learning rate seem too high? too low?\n",
        "\n",
        "(**Hint: it's recommended that you just test a single epoch at a time while you write your training function, to debug and make sure everything is working appropriately.**)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J87zUaXX6p_D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Set a number of training epochs and train your model."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IoOJQt6vFADr"
      },
      "source": [
        "In your training loop, we requested that you store your training loss for each epoch. Using Matplotlib, please plot your training loss as a function of epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xycLVQK-jwiB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### plot loss curve using Matplotlib"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XitAd61L6p_M",
        "colab_type": "text"
      },
      "source": [
        "> **Q6.1)  How do you know when your network is done training?**\n",
        "\n",
        "...\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxomi9w5kxIx",
        "colab_type": "text"
      },
      "source": [
        "Another way to check if your models (HNet and MyNet) are well trained is to plot a few image reconstructions to see how well your models do. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pjS79M0oDj_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# extract 6 figures from training DataLoader\n",
        "mini_batch, _ = next(iter(train_loader))\n",
        "n_examples = min(6, mini_batch.shape[0])\n",
        "examples = mini_batch[:n_examples]\n",
        "\n",
        "# compute reconstructions\n",
        "with torch.no_grad():\n",
        "    reconstr_examples = model.forward(\n",
        "        examples.view(n_examples, -1).to(device))\n",
        "\n",
        "# save image with original v. reconstructed images\n",
        "comparison = torch.cat([\n",
        "    examples,\n",
        "    reconstr_examples.view(-1, 1, 28, 28).cpu()])\n",
        "save_image(comparison.cpu(), 'training_reconstruction.png', nrow=n_examples)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XG_F4XTqwk9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Image('training_reconstruction.png', width=800)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxNZwHPUGxKu",
        "colab_type": "text"
      },
      "source": [
        "> **Q6.2) What does `torch.no_grad()` do?**\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_locd74ddqV",
        "colab_type": "text"
      },
      "source": [
        "### [Only if running in Colab] Download/upload your saved checkpoints\n",
        "\n",
        "**You can skip this section if you are running this notebook locally on your computer.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxvMwSWTs6ML",
        "colab_type": "text"
      },
      "source": [
        "Google Colab stores data in folder `/content`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6RdtYz2n096",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%bash\n",
        "pwd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8SsP_nAstaSl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0ziG5pNst2k",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "If, after some training runs, you feel like you trained a good model and would like to store it, follow the instructions in this subsection. Next time you reopen Colab, you can reupload your best trained model and skip the training step!\n",
        "\n",
        "**Pro tip:** Another option is to link Colab to your personal Google Drive (if you have it). Your Drive folder will then be accessible directly from Colab and you can save your data in it. No need to keep downloading and reuploading files!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fk4vVzfUgtVc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Mount your personal Drive folder in Colab\n",
        "# (follow instructions, you'll need an authorization code)\n",
        "colab.drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkFcoDnv9SXw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Helper functions for downloading/uploading data to Colab\n",
        "\n",
        "import os\n",
        "import google.colab as colab\n",
        "import tarfile\n",
        "\n",
        "def download_files(files):\n",
        "    '''\n",
        "    Download a file, or a list of files, as a \n",
        "    compressed .tar archive named 'backup.tar'\n",
        "    '''\n",
        "    if type(files) not in (list, tuple):\n",
        "        files = [files,]\n",
        "    try:\n",
        "        # compress files\n",
        "        tar = tarfile.open('backup.tar', 'w')\n",
        "        for f in files:\n",
        "            tar.add(f)\n",
        "        tar.close()\n",
        "        # start download\n",
        "        colab.files.download('backup.tar')\n",
        "    except Exception as e:\n",
        "        raise e\n",
        "    finally:\n",
        "        os.remove('backup.tar')\n",
        "    return\n",
        "\n",
        "def upload_files():\n",
        "    '''\n",
        "    Open browser dialogue for uploading files to Colab folder.\n",
        "    If a single .tar archive is uploaded, it will be uncompressed.\n",
        "    '''\n",
        "    d = colab.files.upload()\n",
        "    if len(d) == 1:\n",
        "        filename = list(d.keys())[0]\n",
        "        if filename.endswith('.tar'):\n",
        "            tar = tarfile.open(filename)\n",
        "            tar.extractall()\n",
        "            tar.close()\n",
        "            print(f'{filename} has been uploaded and '\n",
        "                  'uncompressed successfully.')\n",
        "    return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JpmMQy-mA45w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Download file(s):\n",
        "download_files(['model-59.pth', 'train_loss.npy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAO75l40fz-T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Re-upload a file (e.g. 'backup.tar') from your computer to Colab\n",
        "upload_files()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FC8PXHyR6p_Q",
        "colab_type": "text"
      },
      "source": [
        "### 7. Visualize the learning process\n",
        "\n",
        "We'll next try to visualize how well the model is learning on the **test set**. To do this, we'll first visualize the \"learning process\" by viewing reconstruction.\n",
        "\n",
        "* Using your saved models in the last step, plot a batch of original images from their test set, and their corresponding reconstructions based on each of your saved models over time. You should see the quality of the reconstructions improving over time.\n",
        "* To visualize images, you can either use the same approach as for the training examples, or use the helper functions provided below. Note that the latter approach is more correct because it removes the preprocessing transformations introduced when creating the DataLoader.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDO9E6Vl6p_N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Optional Helper Functions for Plotting Multiple Images\n",
        "\n",
        "def imshow(inp, \n",
        "           figsize=(10,10),\n",
        "           mean=0.1307, # for MNIST train\n",
        "           std=0.3081, # for MNIST train\n",
        "           title=None):\n",
        "    \"\"\"Imshow for Tensor.\"\"\"\n",
        "    inp = inp.cpu().detach()\n",
        "    inp = inp.numpy().transpose((1, 2, 0))\n",
        "    mean = np.array(mean)\n",
        "    std = np.array(std)\n",
        "    inp = std * inp + mean\n",
        "    inp = np.clip(inp, 0, 1)\n",
        "    plt.figure(figsize=figsize)\n",
        "    plt.imshow(inp)\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
        "    \n",
        "def reconstructions_from_batch(model, batch):\n",
        "    batch = batch.view(-1, 28 * 28).to(device)\n",
        "    reconstruction = model(batch)\n",
        "    return reconstruction.reshape(batch.shape[0],1,28,28)\n",
        "\n",
        "# Get a batch of training data\n",
        "batch, classes = next(iter(test_loader))\n",
        "\n",
        "# Make a grid from batch\n",
        "out = torchvision.utils.make_grid(batch)\n",
        "imshow(out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2edUEVG3Rev",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Iterate over checkpoints and plot reconstruction \n",
        "### figures from the test set."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfhuSg4D6p_R",
        "colab_type": "text"
      },
      "source": [
        "As discussed in class, the first part of an autoencoder maps the original input into a lower-dimensional latent space. \n",
        "* Just as shown in Hinton and Salakhutdinov, run your test set of 10,000 MNIST digits through the **encoding layer** of one of the trained networks above. Each sample should readily map to a 2-dimension point. To do this, it will be helpful to fill out a new function, **encode** below, that takes in your trained model and your test_dataloader to produce pairs of 2d-latent embeddings and their corresponding labels (e.g. 3, 4).\n",
        "* Plot each point in these two dimensions, and color each point in this **latent space** by their known **labels**. \n",
        "* Does your autoencoder separate out different classes effectively? What classes seem to be closer and what classes are farther apart in this latent space?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNg0PfrK4gzn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Write a helper function to grab examples from the test_loader to generate\n",
        "### pairs of embeddings and their associated labels\n",
        "\n",
        "def encode(model, device, test_loader):\n",
        "  #### Fill this in! ####\n",
        "  latent_embeddings # get the latent embeddings, which will ultimately be a vector of x, y coordinates\n",
        "  labels # this should match the dim of latent_embeddings, so each pair of coordinates has an associated label\n",
        "  return latent_embeddings, labels\n",
        "\n",
        "### Plot latent space representation color-coded \n",
        "### according to their \"true\" labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78___-BU6p_S",
        "colab_type": "text"
      },
      "source": [
        "## Optional: Train an autoencoder on CelebA Faces\n",
        "\n",
        "Real-world images tend to be far more complex than digits from MNIST. As an optional exercise for your own interest, or for students looking for more experience, we'll investigate a subset of CelebA below.\n",
        "\n",
        "We provide the images in a .zip file (`faces.zip`) in the class's Google Drive folder, which contains a \"train\" and \"test\" set of 80k and 10k images, respectively. Although these are color, RGB images, below we've set up the datasets to convert these to grayscale with precomputed means (0.4401) and stds (0.2407), for convenience and easier compute."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1K6o1Wr88EXj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Download faces.zip and unzip it into bmi219_downloads/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nM7WXmAu6p_T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preprocessing = transforms.Compose([\n",
        "    transforms.Grayscale(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4401,), (0.2407,)),\n",
        "])\n",
        "\n",
        "train_dataset = datasets.ImageFolder(\n",
        "    'bmi219_downloads/Faces/train',\n",
        "    transform=preprocessing)\n",
        "\n",
        "test_dataset = datasets.ImageFolder(\n",
        "    'bmi219_downloads/Faces/test',\n",
        "    transform=preprocessing)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQn4cgBW6p_X",
        "colab_type": "text"
      },
      "source": [
        "As above, you'll want to:\n",
        "\n",
        "1. set up your dataloaders and visualize some of the images\n",
        "2. set up your autoencoder network architecture\n",
        "3. define your training criterion and optimizer\n",
        "4. train your network\n",
        "    \n",
        "In this case, you should be able to reuse much of your code from above. Consider a few questions:\n",
        "\n",
        "1. How well do complex images like faces work with a latent dimension of 2?\n",
        "2. Do reconstructions look better with a larger bottleneck?\n",
        "3. What kind of features are poorly reconstructed? What happens to sunglasses, hats, and hands?\n",
        "4. Try sampling the 2-d latent space close to existing examples (by adding some noise...) or randomly. What do the generated images look like?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CH68DGE46p_X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}